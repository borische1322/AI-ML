# -*- coding: utf-8 -*-
"""NNTUT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7kNWbKhnVk0Q69_PLpEB6J3Q8dnsIJQ
"""

import torch
import torchvision
from torchvision import transforms, datasets

train = datasets.MNIST("", train=True, download= True, 
                       transform = transforms.Compose([transforms.ToTensor()]))

test = datasets.MNIST("", train=False, download= True, 
                       transform = transforms.Compose([transforms.ToTensor()]))

trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)

for data in trainset:
  print(data)
  break

x, y = data[0][0], data[1][0]

print(y)

import matplotlib.pyplot as plt

plt.imshow(data[0][0].view(28,28))
plt.show()

total = 0
counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}

for data in trainset:
  Xs, ys = data
  for y in ys:
    counter_dict[int(y)] += 1
    total += 1


print(counter_dict)

for i in counter_dict:
  print(f"{i}: {counter_dict[i]/total * 100}")

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

  def __init__(self):
    super().__init__()
    self.fc1 = nn.Linear(28*28, 64)
    self.fc2 = nn.Linear(64, 64)
    self.fc3 = nn.Linear(64, 64)
    self.fc4 = nn.Linear(64, 10)

  def forward(self, x):
    x = F.relu(self.fc1(x))

    x = F.relu(self.fc2(x))

    x = F.relu(self.fc3(x))

    x = self.fc4(x)

    return F.log_softmax(x, dim= 1)


net = Net()
print(net)

X = torch.rand((28, 28))

X = X.view(-1, 28*28) #or (1, 28* 28), -1 is of any size

output = net(X)

output

import torch.optim as optim

optimizer = optim.Adam(net.parameters(), lr= 0.001)

EPOCHS = 3

for epoch in range(EPOCHS):
  for data in trainset:
    # data is a batch of featuresets and labels
    X, y = data
    net.zero_grad()
    output = net(X.view(-1, 28*28))
    loss = F.nll_loss(output , y) #data output is scalar: use nll loss, data output is one hot vector [0, 0, 1, 0]: use mean-sq error
    loss.backward()
    optimizer.step() #adjust the weights
  print(loss)

correct = 0
total = 0

with torch.no_grad(): #when validating our data, dont want gradients to be calculated since it is out of sample data, no optimising
  for data in testset:
    X, y = data
    output = net(X.view(-1, 28*28))
    for idx, i in enumerate(output):
      if torch.argmax(i) == y[idx]:
        correct += 1
      total += 1


print("Accuracy: ", round(correct/total, 3))

X

import matplotlib.pyplot as plt

plt.imshow(X[4].view(28, 28))

plt.show()

print(torch.argmax(net(X[4].view(-1, 28*28))[0]))

